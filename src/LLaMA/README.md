We leverage an earlier version of the [hiyouga/LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning/) framework for fine-tuning and evaluating LLaMA models.

The script `data/convert_to_llama.py` is used to convert our dataset into a format compatible with this fine-tuning framework.

Metadata for the converted dataset is provided in `data/dataset_info.json`, ensuring it can be effectively utilized by the framework.

For detailed instructions and further information, please refer to the original [LLaMA-Efficient-Tuning repository](https://github.com/hiyouga/LLaMA-Efficient-Tuning/).